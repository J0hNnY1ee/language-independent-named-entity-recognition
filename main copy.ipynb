{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class NerDataset(Dataset):\n",
    "    def __init__(self,data_path,enc):\n",
    "        self.sentences = []\n",
    "        self.targets = []\n",
    "        self.numeric_targets = []\n",
    "        self.tokenized_sentences = []\n",
    "        self.tag_to_id = {}\n",
    "        self.id_to_tag = {}\n",
    "        self.load_data_and_process(data_path,enc)\n",
    "        \n",
    "        \n",
    "    def load_data_and_process(self,tsv_path,enc):\n",
    "        # 打开文件\n",
    "        with open(tsv_path, 'r', encoding='utf-8') as file:\n",
    "            # 读取文件内容\n",
    "            lines = file.readlines()\n",
    "        # 初始化变量\n",
    "        sentences = []  # 存储所有句子的词\n",
    "        targets = []    # 存储所有句子的标注\n",
    "        current_sentence = []  # 存储当前句子的词\n",
    "        current_target = []    # 存储当前句子的标注\n",
    "\n",
    "        # 遍历每一行\n",
    "        for line in lines:\n",
    "            # 去除行首尾的空白字符\n",
    "            line = line.strip()\n",
    "            \n",
    "            # 如果行为空，说明句子结束\n",
    "            if not line:\n",
    "                if current_sentence:  # 如果当前句子不为空\n",
    "                    sentences.append(current_sentence)  # 将当前句子添加到句子列表\n",
    "                    targets.append(current_target)      # 将当前标注添加到标注列表\n",
    "                    current_sentence = []  # 重置当前句子\n",
    "                    current_target = []    # 重置当前标注\n",
    "            else:\n",
    "                # 分割行内容，获取词和标注\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    word, tag = parts\n",
    "                    current_sentence.append(word)  # 将词添加到当前句子\n",
    "                    current_target.append(tag)    # 将标注添加到当前标注\n",
    "                else:\n",
    "                    # 处理可能的格式错误\n",
    "                    print(f\"Warning: Invalid line format - {line}\")\n",
    "        # 检查是否有最后一个句子未被添加\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "            targets.append(current_target)\n",
    "        \n",
    "        self.sentences = sentences\n",
    "        self.targets = targets\n",
    "        \n",
    "        # 创建标签到数字的映射\n",
    "        unique_tags = set(tag for target in targets for tag in target)\n",
    "        tag_to_id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "        id_to_tag = {idx: tag for tag, idx in tag_to_id.items()}  # 创建数字到标签的映射\n",
    "\n",
    "        # 将标注列表转换为数字列表\n",
    "        numeric_targets = []\n",
    "        for target in targets:\n",
    "            numeric_target = [tag_to_id[tag] for tag in target]\n",
    "            numeric_targets.append(numeric_target) \n",
    "        \n",
    "        self.numeric_targets = numeric_targets\n",
    "        self.id_to_tag = id_to_tag\n",
    "        self.tag_to_id = tag_to_id\n",
    "        \n",
    "        \n",
    "        sentences_joined = [' '.join(sentence) for sentence in sentences]\n",
    "        self.tokenized_sentences =  [enc.encode(sentence) for sentence in sentences_joined]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence_tensor = torch.tensor(self.tokenized_sentences[idx], dtype=torch.long)  # 转换为 LongTensor\n",
    "        target_tensor = torch.tensor(self.numeric_targets[idx], dtype=torch.long)  # 目标也转换为 LongTensor\n",
    "        return sentence_tensor, target_tensor\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 双向LSTM层\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        # 线性变换到标签空间\n",
    "        self.linear = nn.Linear(hidden_dim * 2, num_tags)\n",
    "    \n",
    "    def forward(self, input_sentences):\n",
    "        embedded = self.embedding(input_sentences)  # (batch_size, seq_len, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "        emissions = self.linear(lstm_out)  # (batch_size, seq_len, num_tags)\n",
    "        return emissions\n",
    "\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        \n",
    "        self.bilstm = BiLSTM(vocab_size, embedding_dim, hidden_dim, num_tags)\n",
    "        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))\n",
    "        self.start_transitions = nn.Parameter(torch.randn(num_tags))\n",
    "        self.end_transitions = nn.Parameter(torch.randn(num_tags))\n",
    "\n",
    "    def forward(self, sentences,tags,is_train):\n",
    "        emissions = self.bilstm(sentences)\n",
    "        if is_train:\n",
    "            loss = self.log_likelihood(emissions, tags)\n",
    "            return -loss  # Negative log likelihood for loss optimization\n",
    "        else:\n",
    "            loss = self.log_likelihood(emissions, tags)\n",
    "            \n",
    "            return self.decode(emissions),-loss\n",
    "        \n",
    "        \n",
    "    def log_likelihood(self,emissions,tags):\n",
    "        batch_size, seq_len, _ = emissions.shape\n",
    "        print(emissions.shape)\n",
    "        mask = tags != -1  # Mask for valid positions\n",
    "        score = self.start_transitions[tags[:, 0]]\n",
    "        score += emissions[torch.arange(batch_size), 0, tags[:, 0]]\n",
    "        for i in range(1, seq_len):\n",
    "            valid = mask[:, i]\n",
    "            previous_tags = tags[:, i - 1][valid]\n",
    "            current_tags = tags[:, i][valid]\n",
    "            score[valid] += self.transitions[previous_tags, current_tags] + emissions[torch.arange(batch_size)[valid], i, current_tags]\n",
    "        last_tags = tags[torch.arange(batch_size), mask.sum(dim=1) - 1]\n",
    "        score += self.end_transitions[last_tags]\n",
    "        \n",
    "        partition = self.compute_partition_function(emissions, mask)# 总的概率\n",
    "        return torch.sum(score - partition)\n",
    "    \n",
    "    \n",
    "    def compute_partition_function(self, emissions, mask):\n",
    "        batch_size, seq_len, num_tags = emissions.shape()\n",
    "        log_alpha = self.start_transitions + emissions[:, 0]\n",
    "        for i in range(1, seq_len):\n",
    "            valid = mask[:, i]\n",
    "            new_log_alpha = torch.logsumexp(log_alpha.unsqueeze(2) + self.transitions + emissions[:, i].unsqueeze(1), dim=1)\n",
    "            log_alpha[valid] = new_log_alpha[valid]\n",
    "        return torch.logsumexp(log_alpha + self.end_transitions, dim=1)\n",
    "    \n",
    "    def decode(self, emissions):\n",
    "        batch_size, seq_len, _ = emissions.shape()\n",
    "        \n",
    "        backpointers = []\n",
    "        \n",
    "        # Initialize the viterbi scores\n",
    "        viterbi_scores = self.start_transitions + emissions[:, 0]\n",
    "        \n",
    "        for i in range(1, seq_len):\n",
    "            viterbi_scores, best_tags = torch.max(viterbi_scores.unsqueeze(2) + self.transitions, dim=1)\n",
    "            viterbi_scores += emissions[:, i]\n",
    "            backpointers.append(best_tags)\n",
    "        \n",
    "        # Backtrack to get the best path\n",
    "        best_last_tag = torch.argmax(viterbi_scores + self.end_transitions, dim=1)\n",
    "        best_tags = [best_last_tag]\n",
    "        \n",
    "        for backpointer in reversed(backpointers):\n",
    "            best_last_tag = backpointer[torch.arange(batch_size), best_last_tag]\n",
    "            best_tags.insert(0, best_last_tag)\n",
    "        \n",
    "        return torch.stack(best_tags, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, learning_rate=0.001, num_epochs=1, lr_scheduler=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for sentences, tags in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(sentences, tags, is_train=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if lr_scheduler:\n",
    "                lr_scheduler.step()  # 调整学习率\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, eva_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentences, tags in eva_loader:\n",
    "            predictions, loss = model(sentences, is_train=False, tags=tags)  # 调整调用方式\n",
    "            total_loss += loss.item()\n",
    "            all_predictions.extend(predictions)\n",
    "            \n",
    "            # 计算正确率\n",
    "            for pred, true_tags in zip(predictions, tags):\n",
    "                correct += sum(p == t for p, t in zip(pred, true_tags))\n",
    "                total += len(true_tags)\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return total_loss / len(eva_loader), all_predictions, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Encoding' object has no attribute 'encode_with_unstable_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\n\u001b[32m      3\u001b[39m enc = tiktoken.get_encoding(\u001b[33m\"\u001b[39m\u001b[33mgpt2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m enc = \u001b[43menc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_with_unstable_mode\u001b[49m()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(enc.encode(\u001b[33m\"\u001b[39m\u001b[33miojaoisdio\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# dataset = NerDataset('eng.dev.tsv',enc)\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# # 创建 DataLoader\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# train(model, train_loader, num_epochs=1)\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Encoding' object has no attribute 'encode_with_unstable_mode'"
     ]
    }
   ],
   "source": [
    "\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"iojaoisdio\"))\n",
    "# dataset = NerDataset('eng.dev.tsv',enc)\n",
    "\n",
    "# # 创建 DataLoader\n",
    "# vocab_size = 50257\n",
    "# embedding_dim = 128\n",
    "# hidden_dim = 128\n",
    "# num_tags = len(dataset.tag_to_id)\n",
    "\n",
    "# train_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "# model = BiLSTM_CRF(vocab_size, embedding_dim, hidden_dim, num_tags)\n",
    "\n",
    "# train(model, train_loader, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
