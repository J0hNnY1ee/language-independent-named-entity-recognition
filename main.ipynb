{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_data_and_process( tsv_path):\n",
    "    # 打开文件\n",
    "    with open(tsv_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        # 读取文件内容\n",
    "        lines = file.readlines()\n",
    "    # 初始化变量\n",
    "    sentences = []  # 存储所有句子的词\n",
    "    targets = []  # 存储所有句子的标注\n",
    "    current_sentence = []  # 存储当前句子的词\n",
    "    current_target = []  # 存储当前句子的标注\n",
    "\n",
    "    # 遍历每一行\n",
    "    for line in lines:\n",
    "        # 去除行首尾的空白字符\n",
    "        line = line.strip()\n",
    "\n",
    "        # 如果行为空，说明句子结束\n",
    "        if not line:\n",
    "            if current_sentence:  # 如果当前句子不为空\n",
    "                sentences.append(current_sentence)  # 将当前句子添加到句子列表\n",
    "                targets.append(current_target)  # 将当前标注添加到标注列表\n",
    "                current_sentence = []  # 重置当前句子\n",
    "                current_target = []  # 重置当前标注\n",
    "        else:\n",
    "            # 分割行内容，获取词和标注\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                word, tag = parts\n",
    "                current_sentence.append(word)  # 将词添加到当前句子\n",
    "                current_target.append(tag)  # 将标注添加到当前标注\n",
    "            else:\n",
    "                # 处理可能的格式错误\n",
    "                print(f\"Warning: Invalid line format - {line}\")\n",
    "    # 检查是否有最后一个句子未被添加\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        targets.append(current_target)\n",
    "\n",
    "    return sentences, targets\n",
    "\n",
    "\n",
    "class NerDataset(Dataset):\n",
    "    def __init__(self, data_path, word_to_id, tag_to_id):\n",
    "        self.sentences = []\n",
    "        self.targets = []\n",
    "        self.numeric_targets = []\n",
    "        self.tokenized_sentences = []\n",
    "        self.tag_to_id = tag_to_id\n",
    "        self.word_to_id = word_to_id\n",
    "        self.sentences, self.targets = load_data_and_process(data_path)\n",
    "        self.process_data()\n",
    "    def process_data(self):\n",
    "\n",
    "        # 将句子中的单词转换为对应的数字ID\n",
    "        tokenized_sentences = []\n",
    "        for sentence in self.sentences:\n",
    "            tokenized_sentence = [self.word_to_id[word] for word in sentence]\n",
    "            tokenized_sentences.append(tokenized_sentence)\n",
    "\n",
    "        # 将标注列表转换为数字列表\n",
    "        numeric_targets = []\n",
    "        for target in self.targets:\n",
    "            numeric_target = [self.tag_to_id[tag] for tag in target]\n",
    "            numeric_targets.append(numeric_target)\n",
    "\n",
    "        self.numeric_targets = numeric_targets\n",
    "        self.tokenized_sentences = tokenized_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence_tensor = torch.tensor(\n",
    "            self.tokenized_sentences[idx], dtype=torch.long\n",
    "        )  # 转换为 LongTensor\n",
    "        target_tensor = torch.tensor(\n",
    "            self.numeric_targets[idx], dtype=torch.long\n",
    "        )  # 目标也转换为 LongTensor\n",
    "        return sentence_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 双向LSTM层\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        # 线性变换到标签空间\n",
    "        self.linear = nn.Linear(hidden_dim * 2, num_tags)\n",
    "\n",
    "    def forward(self, input_sentences):\n",
    "        embedded = self.embedding(\n",
    "            input_sentences\n",
    "        )  # (batch_size, seq_len, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "        emissions = self.linear(lstm_out)  # (batch_size, seq_len, num_tags)\n",
    "        return emissions\n",
    "\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "\n",
    "        self.bilstm = BiLSTM(vocab_size, embedding_dim, hidden_dim, num_tags)\n",
    "        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))\n",
    "        self.start_transitions = nn.Parameter(torch.randn(num_tags))\n",
    "        self.end_transitions = nn.Parameter(torch.randn(num_tags))\n",
    "\n",
    "    def forward(self, sentences, tags, is_train):\n",
    "        emissions = self.bilstm(sentences)\n",
    "        if is_train:\n",
    "            loss = self.log_likelihood(emissions, tags)\n",
    "            return -loss  # Negative log likelihood for loss optimization\n",
    "        else:\n",
    "            loss = self.log_likelihood(emissions, tags)\n",
    "\n",
    "            return self.decode(emissions), -loss\n",
    "\n",
    "    def log_likelihood(self, emissions, tags):\n",
    "        batch_size, seq_len, _ = emissions.shape\n",
    "        mask = tags != -1  # Mask for valid positions\n",
    "        score = self.start_transitions[tags[:, 0]]\n",
    "        score += emissions[torch.arange(batch_size), 0, tags[:, 0]]\n",
    "        for i in range(1, seq_len):\n",
    "            valid = mask[:, i]\n",
    "            previous_tags = tags[:, i - 1][valid]\n",
    "            current_tags = tags[:, i][valid]\n",
    "            score[valid] += (\n",
    "                self.transitions[previous_tags, current_tags]\n",
    "                + emissions[torch.arange(batch_size)[valid], i, current_tags]\n",
    "            )\n",
    "        last_tags = tags[torch.arange(batch_size), mask.sum(dim=1) - 1]\n",
    "        score += self.end_transitions[last_tags]\n",
    "\n",
    "        partition = self.compute_partition_function(emissions, mask)  # 总的概率\n",
    "        return torch.sum(score - partition)\n",
    "\n",
    "    def compute_partition_function(self, emissions, mask):\n",
    "        batch_size, seq_len, num_tags = emissions.shape\n",
    "        log_alpha = self.start_transitions + emissions[:, 0]\n",
    "        for i in range(1, seq_len):\n",
    "            valid = mask[:, i]\n",
    "            new_log_alpha = torch.logsumexp(\n",
    "                log_alpha.unsqueeze(2)\n",
    "                + self.transitions\n",
    "                + emissions[:, i].unsqueeze(1),\n",
    "                dim=1,\n",
    "            )\n",
    "            log_alpha[valid] = new_log_alpha[valid]\n",
    "        return torch.logsumexp(log_alpha + self.end_transitions, dim=1)\n",
    "\n",
    "    def decode(self, emissions):\n",
    "        batch_size, seq_len, _ = emissions.shape\n",
    "\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi scores\n",
    "        viterbi_scores = self.start_transitions + emissions[:, 0]\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            viterbi_scores, best_tags = torch.max(\n",
    "                viterbi_scores.unsqueeze(2) + self.transitions, dim=1\n",
    "            )\n",
    "            viterbi_scores += emissions[:, i]\n",
    "            backpointers.append(best_tags)\n",
    "\n",
    "        # Backtrack to get the best path\n",
    "        best_last_tag = torch.argmax(viterbi_scores + self.end_transitions, dim=1)\n",
    "        best_tags = [best_last_tag]\n",
    "\n",
    "        for backpointer in reversed(backpointers):\n",
    "            best_last_tag = backpointer[torch.arange(batch_size), best_last_tag]\n",
    "            best_tags.insert(0, best_last_tag)\n",
    "\n",
    "        return torch.stack(best_tags, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, learning_rate=0.001, num_epochs=1, lr_scheduler=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for sentences, tags in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(sentences, tags, is_train=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if lr_scheduler:\n",
    "                lr_scheduler.step()  # 调整学习率\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, eva_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentences, tags in eva_loader:\n",
    "            predictions, loss = model(sentences, is_train=False, tags=tags)  # 调整调用方式\n",
    "            total_loss += loss.item()\n",
    "            all_predictions.extend(predictions)\n",
    "            \n",
    "            # 计算正确率\n",
    "            for pred, true_tags in zip(predictions, tags):\n",
    "                correct += sum(p == t for p, t in zip(pred, true_tags))\n",
    "                total += len(true_tags)\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return total_loss / len(eva_loader), all_predictions, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_data(file_paths):\n",
    "    sentences = []\n",
    "    targets = []\n",
    "    for file_path in file_paths:\n",
    "        sentence, target = load_data_and_process(file_path)\n",
    "        sentences.extend(sentence)\n",
    "        targets.extend(target)\n",
    "    return sentences, targets\n",
    "\n",
    "\n",
    "def process_data(sentences, targets):\n",
    "    \"\"\"\n",
    "    对数据进行处理，包括创建标签和单词的映射，将标注和句子转换为数字形式，并获取词汇表大小。\n",
    "\n",
    "    参数:\n",
    "        sentences (list): 所有句子的单词列表，每个句子是一个单词列表。\n",
    "        targets (list): 所有句子的标注列表，每个句子的标注是一个列表。\n",
    "\n",
    "    返回:\n",
    "        tuple: 包含以下元素的元组\n",
    "            - tag_to_id (dict): 标签到ID的映射字典。\n",
    "            - id_to_tag (dict): ID到标签的映射字典。\n",
    "            - numeric_targets (list): 数字形式的标注列表。\n",
    "            - word_to_id (dict): 单词到ID的映射字典。\n",
    "            - id_to_word (dict): ID到单词的映射字典。\n",
    "            - tokenized_sentences (list): 数字形式的句子列表。\n",
    "            - vocab_size (int): 词汇表的大小。\n",
    "    \"\"\"\n",
    "    # 创建标签到ID和ID到标签的映射\n",
    "    unique_tags = set(tag for target in targets for tag in target)\n",
    "    tag_to_id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "    id_to_tag = {idx: tag for tag, idx in tag_to_id.items()}\n",
    "\n",
    "    # 将标注列表转换为数字列表\n",
    "    numeric_targets = []\n",
    "    for target in targets:\n",
    "        numeric_target = [tag_to_id[tag] for tag in target]\n",
    "        numeric_targets.append(numeric_target)\n",
    "\n",
    "    # 创建单词到ID和ID到单词的映射\n",
    "    unique_words = set(word for sentence in sentences for word in sentence)\n",
    "    word_to_id = {word: idx for idx, word in enumerate(unique_words)}\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "\n",
    "    # 将句子中的单词转换为对应的数字ID\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = [word_to_id[word] for word in sentence]\n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "\n",
    "    # 获取词汇表的大小\n",
    "    vocab_size = len(word_to_id)\n",
    "\n",
    "    return (\n",
    "        tag_to_id,\n",
    "        id_to_tag,\n",
    "        numeric_targets,\n",
    "        word_to_id,\n",
    "        id_to_word,\n",
    "        tokenized_sentences,\n",
    "        vocab_size,\n",
    "    )\n",
    "\n",
    "\n",
    "# 使用函数加载数据\n",
    "file_paths = [\"eng.dev.tsv\", \"eng.test.tsv\", \"eng.train.tsv\"]\n",
    "sentences, targets = load_and_merge_data(file_paths)  # 创建标签到数字的映射\n",
    "tag_to_id, id_to_tag, numeric_targets, word_to_id, id_to_word, tokenized_sentences, vocab_size = process_data(sentences, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NerDataset('eng.train.tsv',word_to_id,tag_to_id)\n",
    "\n",
    "# 创建 DataLoader\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "num_tags = len(train_dataset.tag_to_id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "model = BiLSTM_CRF(vocab_size, embedding_dim, hidden_dim, num_tags)\n",
    "\n",
    "train(model, train_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset  = NerDataset('eng.test.tsv',word_to_id,tag_to_id)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "evaluate(model,test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
